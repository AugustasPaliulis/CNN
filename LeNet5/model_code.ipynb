{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x) applied elementwise.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"tanh activation: (e^x - e^{-x})/(e^x + e^{-x}) applied elementwise.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation for output layer (assumes x is 1D or 2D batch of scores).\"\"\"\n",
    "    # For numeric stability, subtract max\n",
    "    x_shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[7.74466202 7.87499121 6.46599859 ... 7.16033799 7.23879439 7.57612002]\n",
      "  [6.7638502  7.60400893 6.76476533 ... 7.40983525 7.83721912 7.51518527]\n",
      "  [7.7023605  7.90636587 8.46711908 ... 7.1873372  5.81481626 6.23583491]\n",
      "  ...\n",
      "  [6.2697161  6.11070863 7.97203259 ... 5.69440666 5.42024257 4.96823978]\n",
      "  [6.58880314 5.96099481 7.55421972 ... 4.98563466 5.21394182 4.52339772]\n",
      "  [6.48581985 5.82830623 8.03175372 ... 6.29752652 5.23555688 6.29593642]]\n",
      "\n",
      " [[5.93311404 7.44329115 6.54410268 ... 6.66468612 6.09950863 6.35260915]\n",
      "  [6.96811135 6.81049245 6.22031258 ... 6.19732251 6.06148532 6.96708839]\n",
      "  [6.88897563 6.53026876 6.79292122 ... 5.9612273  6.28533861 6.0379001 ]\n",
      "  ...\n",
      "  [6.57106862 5.69265337 6.36471862 ... 4.44734034 5.33227731 4.17142554]\n",
      "  [5.92534058 4.65587737 6.39839314 ... 4.34143261 4.75927957 4.72974404]\n",
      "  [6.60682917 5.59506476 5.83736486 ... 4.37604685 5.76264114 5.0160156 ]]\n",
      "\n",
      " [[8.09944941 8.61101523 7.09485369 ... 7.38722896 7.70363995 8.54063998]\n",
      "  [7.30421841 6.78371162 7.21461147 ... 8.2346193  7.9148503  8.4094447 ]\n",
      "  [8.7732494  8.74794721 8.04411942 ... 6.64703698 6.04880518 6.81899275]\n",
      "  ...\n",
      "  [6.89361955 6.96262889 6.93312616 ... 5.66008488 5.81121275 5.45312364]\n",
      "  [6.87939839 6.81516789 7.24909708 ... 5.37573598 5.3812722  5.64575834]\n",
      "  [7.02167289 6.47528992 7.74945206 ... 5.78130924 6.04603196 5.26759299]]\n",
      "\n",
      " [[8.5961334  6.78596265 6.35491048 ... 7.32474828 7.80483156 7.7376731 ]\n",
      "  [6.86098488 7.36517477 7.10373009 ... 7.70926608 6.84430875 7.62403997]\n",
      "  [8.66464129 8.22388589 7.58644556 ... 6.03018512 6.18329934 7.10502352]\n",
      "  ...\n",
      "  [6.5399569  6.56160141 6.68461997 ... 5.5784778  4.30221018 5.01941266]\n",
      "  [5.98756215 6.53838223 6.66190118 ... 6.06618573 5.0418618  5.36379809]\n",
      "  [6.38566258 7.7692075  7.03481739 ... 6.26088807 5.05217197 5.28374223]]\n",
      "\n",
      " [[7.68448321 8.35525419 8.01080512 ... 8.24792749 7.7632817  7.80773499]\n",
      "  [8.21819721 8.20641423 7.27984568 ... 7.59502424 7.26345111 7.96905695]\n",
      "  [8.53492203 8.48306706 8.9429081  ... 7.40586042 6.50660342 6.90345193]\n",
      "  ...\n",
      "  [7.14833871 6.10516631 8.15613238 ... 5.71851839 5.59295053 5.67726241]\n",
      "  [7.39528066 7.04365904 8.10999726 ... 6.35288697 5.16754053 5.23184844]\n",
      "  [7.48484436 7.472025   7.51068125 ... 5.48859607 6.42811993 6.3388154 ]]\n",
      "\n",
      " [[7.94076877 7.52689433 6.49742154 ... 7.12253178 7.66796957 7.80057306]\n",
      "  [7.21335836 7.43066438 6.6396889  ... 7.8136451  6.74916129 7.39933531]\n",
      "  [8.13813647 8.36020254 8.13642437 ... 5.9232187  6.20575485 6.99330353]\n",
      "  ...\n",
      "  [6.36655392 6.61571493 7.23215929 ... 5.35326247 5.30802511 4.89786035]\n",
      "  [6.44330665 5.61305671 6.99999542 ... 5.53709348 5.44702253 5.27116473]\n",
      "  [6.58737497 6.9740377  7.13133567 ... 5.59094272 6.10622695 5.33160352]]]\n",
      "<><><><><\n",
      "[[[0.99999962 0.99999971 0.99999516 ... 0.99999879 0.99999897 0.99999947]\n",
      "  [0.99999733 0.9999995  0.99999734 ... 0.99999927 0.99999969 0.99999941]\n",
      "  [0.99999959 0.99999973 0.99999991 ... 0.99999886 0.9999822  0.99999233]\n",
      "  ...\n",
      "  [0.99999283 0.99999015 0.99999976 ... 0.99997736 0.99996082 0.99990325]\n",
      "  [0.99999621 0.99998671 0.99999945 ... 0.99990656 0.99994081 0.99976449]\n",
      "  [0.99999535 0.99998268 0.99999979 ... 0.99999322 0.99994331 0.9999932 ]]\n",
      "\n",
      " [[0.99998595 0.99999931 0.99999586 ... 0.99999675 0.99998993 0.99999393]\n",
      "  [0.99999823 0.99999757 0.99999209 ... 0.99999172 0.99998913 0.99999822]\n",
      "  [0.99999792 0.99999574 0.99999748 ... 0.99998672 0.99999306 0.99998861]\n",
      "  ...\n",
      "  [0.99999608 0.99997728 0.99999407 ... 0.9997258  0.99995328 0.99952393]\n",
      "  [0.99998573 0.9998193  0.99999446 ... 0.99966113 0.99985306 0.99984412]\n",
      "  [0.99999635 0.99997238 0.99998299 ... 0.99968379 0.99998025 0.99991207]]\n",
      "\n",
      " [[0.99999982 0.99999993 0.99999862 ... 0.99999923 0.99999959 0.99999992]\n",
      "  [0.99999909 0.99999744 0.99999892 ... 0.99999986 0.99999973 0.9999999 ]\n",
      "  [0.99999995 0.99999995 0.99999979 ... 0.99999663 0.99998885 0.99999761]\n",
      "  ...\n",
      "  [0.99999794 0.99999821 0.9999981  ... 0.99997575 0.99998207 0.99996331]\n",
      "  [0.99999788 0.99999759 0.99999899 ... 0.99995717 0.99995764 0.99997504]\n",
      "  [0.99999841 0.99999525 0.99999963 ... 0.99998097 0.99998879 0.99994683]]\n",
      "\n",
      " [[0.99999993 0.99999745 0.99999396 ... 0.99999913 0.99999967 0.99999962]\n",
      "  [0.9999978  0.9999992  0.99999865 ... 0.9999996  0.99999773 0.99999952]\n",
      "  [0.99999994 0.99999986 0.99999949 ... 0.99998843 0.99999148 0.99999865]\n",
      "  ...\n",
      "  [0.99999583 0.999996   0.99999688 ... 0.99997145 0.99963348 0.99991266]\n",
      "  [0.9999874  0.99999581 0.99999673 ... 0.99998924 0.9999165  0.99995614]\n",
      "  [0.99999432 0.99999964 0.99999845 ... 0.99999271 0.9999182  0.99994852]]\n",
      "\n",
      " [[0.99999958 0.99999989 0.99999978 ... 0.99999986 0.99999964 0.99999967]\n",
      "  [0.99999985 0.99999985 0.99999905 ... 0.99999949 0.99999902 0.99999976]\n",
      "  [0.99999992 0.99999991 0.99999997 ... 0.99999926 0.99999554 0.99999798]\n",
      "  ...\n",
      "  [0.99999876 0.99999004 0.99999984 ... 0.99997842 0.99997226 0.99997657]\n",
      "  [0.99999925 0.99999848 0.99999982 ... 0.99999393 0.99993505 0.99994289]\n",
      "  [0.99999937 0.99999935 0.9999994  ... 0.99996583 0.99999478 0.99999376]]\n",
      "\n",
      " [[0.99999975 0.99999942 0.99999546 ... 0.9999987  0.99999956 0.99999966]\n",
      "  [0.99999891 0.9999993  0.99999658 ... 0.99999967 0.99999725 0.99999925]\n",
      "  [0.99999983 0.99999989 0.99999983 ... 0.99998567 0.99999186 0.99999831]\n",
      "  ...\n",
      "  [0.9999941  0.99999641 0.99999895 ... 0.9999552  0.99995096 0.99988863]\n",
      "  [0.99999494 0.99997336 0.99999834 ... 0.99996899 0.99996286 0.99994721]\n",
      "  [0.9999962  0.99999825 0.99999872 ... 0.99997215 0.99999006 0.99995322]]]\n"
     ]
    }
   ],
   "source": [
    "def conv_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Convolutional layer forward pass.\n",
    "    X: input array of shape (C_in, H_in, W_in)\n",
    "    W: weight/filter array of shape (C_out, C_in, kH, kW)\n",
    "    b: bias array of shape (C_out,)\n",
    "    Returns: output feature map of shape (C_out, H_out, W_out)\n",
    "\n",
    "    TODO: use numpy for simplifying for loops\n",
    "    \"\"\"\n",
    "    C_out, C_in, kH, kW = W.shape\n",
    "    _, H_in, W_in = X.shape\n",
    "    H_out = H_in - kH + 1  # using stride 1, no padding\n",
    "    W_out = W_in - kW + 1\n",
    "    # Initialize output volume\n",
    "    out = np.zeros((C_out, H_out, W_out))\n",
    "    # Convolution: slide each filter over the input\n",
    "    for oc in range(C_out):             # for each output channel (filter)\n",
    "        for i in range(H_out):          # slide vertically\n",
    "            for j in range(W_out):      # slide horizontally\n",
    "                # current region of input of shape (C_in, kH, kW)\n",
    "                region = X[:, i:i+kH, j:j+kW]\n",
    "                # element-wise multiply and sum -> dot product\n",
    "                out[oc, i, j] = np.sum(region * W[oc]) + b[oc]\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "# X has shape (1, 32, 32) for a single grayscale image, W shape (6, 1, 5, 5), b shape (6,)\n",
    "X = np.random.rand(1, 32, 32)  # dummy input\n",
    "W1 = np.random.rand(6, 1, 5, 5)\n",
    "b1 = np.random.rand(6,)\n",
    "out1 = conv_forward(X, W1, b1)\n",
    "print(out1)  # should be (6, 28, 28)\n",
    "\n",
    "out1_act = tanh(out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "def avg_pool_forward(X):\n",
    "    \"\"\"\n",
    "    Average pooling forward pass (2x2 pool, stride 2).\n",
    "    X: input array of shape (C, H, W)\n",
    "    Returns: output array of shape (C, H/2, W/2)\n",
    "    \"\"\"\n",
    "    C, H, W = X.shape\n",
    "    # Assuming H and W are even and divisible by 2 for simplicity\n",
    "    out = np.zeros((C, H//2, W//2))\n",
    "    for c in range(C):\n",
    "        for i in range(0, H, 2):         # step by 2\n",
    "            for j in range(0, W, 2):     # step by 2\n",
    "                patch = X[c, i:i+2, j:j+2]        # 2x2 region\n",
    "                out[c, i//2, j//2] = np.mean(patch)\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "X_pool_in = np.random.rand(6, 28, 28)  # e.g. output from conv layer (6,28,28)\n",
    "out_pool = avg_pool_forward(X_pool_in)\n",
    "print(out_pool.shape)  # should be (6, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84,)\n"
     ]
    }
   ],
   "source": [
    "def fc_forward(x, W, b):\n",
    "    \"\"\"\n",
    "    Fully-connected layer forward pass.\n",
    "    x: input vector of shape (N_in,)\n",
    "    W: weight matrix of shape (N_out, N_in)\n",
    "    b: bias vector of shape (N_out,)\n",
    "    Returns: output vector of shape (N_out,)\n",
    "    \"\"\"\n",
    "    return W.dot(x) + b\n",
    "\n",
    "# Example usage:\n",
    "x = np.random.rand(120,)    # input from previous layer (flattened conv output)\n",
    "W4 = np.random.rand(84, 120)\n",
    "b4 = np.random.rand(84,)\n",
    "out_fc = fc_forward(x, W4, b4)\n",
    "print(out_fc.shape)  # should be (84,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_forward(image):\n",
    "    \"\"\"\n",
    "    Forward pass for the entire LeNet-5 model on a single image.\n",
    "    image: input image array of shape (1, 32, 32)  - 1 channel, 32x32 (assuming already zero-padded to 32x32 if originally 28x28).\n",
    "    Returns: output probabilities for 10 classes (softmax output vector of shape (10,))\n",
    "    \"\"\"\n",
    "    # Layer C1: Conv 5x5 -> 6 feature maps, then activation\n",
    "    out_c1 = conv_forward(image, W1, b1)         # shape (6, 28, 28)\n",
    "    out_c1 = tanh(out_c1)                        # apply tanh or relu\n",
    "    \n",
    "    # Layer S2: 2x2 Pooling -> 6 feature maps\n",
    "    out_s2 = avg_pool_forward(out_c1)            # shape (6, 14, 14)\n",
    "    \n",
    "    # Layer C3: Conv 5x5 -> 16 feature maps, then activation\n",
    "    out_c3 = conv_forward(out_s2, W2, b2)        # shape (16, 10, 10)\n",
    "    out_c3 = tanh(out_c3)\n",
    "    \n",
    "    # Layer S4: 2x2 Pooling -> 16 feature maps\n",
    "    out_s4 = avg_pool_forward(out_c3)            # shape (16, 5, 5)\n",
    "    \n",
    "    # Layer C5: Conv 5x5 -> 120 feature maps (1x1 each), then activation\n",
    "    out_c5 = conv_forward(out_s4, W3, b3)        # shape (120, 1, 1)\n",
    "    out_c5 = tanh(out_c5)\n",
    "    # Flatten output of C5 to a vector of length 120:\n",
    "    out_c5_flat = out_c5.reshape(-1)             # shape (120,)\n",
    "    \n",
    "    # Layer F6: Fully connected -> 84, then activation\n",
    "    out_f6 = fc_forward(out_c5_flat, W4, b4)     # shape (84,)\n",
    "    out_f6 = tanh(out_f6)\n",
    "    \n",
    "    # Output layer: Fully connected -> 10, then softmax\n",
    "    out_out = fc_forward(out_f6, W5, b5)         # shape (10,)\n",
    "    probs = softmax(out_out)                     # shape (10,) probabilities\n",
    "    \n",
    "    return probs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
