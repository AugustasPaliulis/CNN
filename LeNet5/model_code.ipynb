{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x) applied elementwise.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"tanh activation: (e^x - e^{-x})/(e^x + e^{-x}) applied elementwise.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation for output layer (assumes x is 1D or 2D batch of scores).\"\"\"\n",
    "    # For numeric stability, subtract max\n",
    "    x_shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 8.65269689  9.42854675  8.95199431 ...  8.74085145  9.24134948\n",
      "    9.55777101]\n",
      "  [ 8.82670369  9.53657871  9.78077039 ...  8.09002482  7.8895319\n",
      "    7.93820106]\n",
      "  [ 9.2575507   9.44096558  8.817653   ...  7.8226503   7.86533669\n",
      "    7.35286737]\n",
      "  ...\n",
      "  [ 7.98316966  8.64130349  7.83604953 ...  7.82073939  7.49803918\n",
      "    7.97315672]\n",
      "  [ 7.49586556  8.15670343  8.11818799 ...  7.86417731  7.86818697\n",
      "    7.27293307]\n",
      "  [ 8.25721098  8.72984269  8.56161828 ...  8.60501633  8.44472628\n",
      "    7.9733721 ]]\n",
      "\n",
      " [[ 7.18332206  7.70161094  7.4187746  ...  7.5267895   8.05437829\n",
      "    6.9330012 ]\n",
      "  [ 7.67976425  8.18695511  7.58049322 ...  7.00137078  7.5586372\n",
      "    6.63283405]\n",
      "  [ 7.98354905  8.9939599   7.91851696 ...  6.71862261  7.09160049\n",
      "    6.51825187]\n",
      "  ...\n",
      "  [ 6.67493552  7.3215337   7.05382329 ...  7.19265886  6.15742912\n",
      "    6.21267981]\n",
      "  [ 6.25791461  7.55573582  6.86366544 ...  6.77531094  6.50329463\n",
      "    6.59781375]\n",
      "  [ 6.12297282  7.19130921  7.03384585 ...  7.20716243  6.97956501\n",
      "    6.88784383]]\n",
      "\n",
      " [[ 9.01317689  9.65090169  9.62282276 ... 10.28414286 10.31975365\n",
      "    9.36712719]\n",
      "  [ 9.18735486  9.77111143 10.07367148 ...  9.2818999   8.85880984\n",
      "    7.96438396]\n",
      "  [ 9.96725165 10.09656546  9.85701022 ...  8.82769499  8.9806096\n",
      "    8.16248163]\n",
      "  ...\n",
      "  [ 8.34104429  9.46125607  8.4871946  ...  8.08304462  8.20555904\n",
      "    8.43361039]\n",
      "  [ 8.15016158  8.93402294  8.71431087 ...  8.87518402  8.20429802\n",
      "    8.48139447]\n",
      "  [ 8.04098669  9.71150768  9.96597885 ...  8.94964963  8.9658619\n",
      "    9.27358   ]]\n",
      "\n",
      " [[ 8.06716761  7.93489759  9.01472305 ...  9.02132256  8.59663481\n",
      "    8.70365935]\n",
      "  [ 8.48378153  8.81226064  8.58732691 ...  8.07137789  7.83515187\n",
      "    7.20664146]\n",
      "  [ 8.76887191  8.68522641  8.61225901 ...  8.01671755  7.73081561\n",
      "    7.63183863]\n",
      "  ...\n",
      "  [ 7.69963929  7.92449452  7.4478404  ...  6.57610704  7.9791077\n",
      "    8.01278484]\n",
      "  [ 7.57965946  8.06386828  7.76749175 ...  8.35357639  7.28049001\n",
      "    7.62914702]\n",
      "  [ 7.84208095  8.00240714  9.3056362  ...  7.06497397  8.45234357\n",
      "    7.0277913 ]]\n",
      "\n",
      " [[ 7.49578886  7.1861832   7.97342239 ...  9.22216954  8.40735606\n",
      "    7.81716041]\n",
      "  [ 8.2945405   7.16350919  8.51698578 ...  8.43621704  7.61905945\n",
      "    6.28713797]\n",
      "  [ 8.23929654  8.78553105  8.30255502 ...  7.55435095  7.05835437\n",
      "    7.04590278]\n",
      "  ...\n",
      "  [ 7.92315242  7.70877872  7.26039034 ...  5.74865754  7.25052028\n",
      "    6.54127316]\n",
      "  [ 6.904883    7.75419036  7.28594308 ...  7.11661248  6.31127229\n",
      "    8.218971  ]\n",
      "  [ 6.58353808  8.07549424  8.62264937 ...  6.80016614  7.51422261\n",
      "    7.30926866]]\n",
      "\n",
      " [[ 5.80720155  7.40205425  6.99198596 ...  6.33649093  6.59413642\n",
      "    7.28598803]\n",
      "  [ 6.57249577  7.50684761  7.12032826 ...  5.82841195  5.43545055\n",
      "    6.0758142 ]\n",
      "  [ 7.26251569  7.31799945  7.13907058 ...  5.65837412  5.89429438\n",
      "    5.34755099]\n",
      "  ...\n",
      "  [ 6.36637302  6.1341048   5.29699361 ...  6.51257629  6.22074376\n",
      "    5.85178296]\n",
      "  [ 6.7348346   5.60224627  5.7534352  ...  5.44881431  6.40999103\n",
      "    5.41490004]\n",
      "  [ 5.44916186  6.18307411  6.45899848 ...  6.91002723  5.44869333\n",
      "    6.01252895]]]\n"
     ]
    }
   ],
   "source": [
    "def conv_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Convolutional layer forward pass.\n",
    "    X: input array of shape (C_in, H_in, W_in)\n",
    "    W: weight/filter array of shape (C_out, C_in, kH, kW)\n",
    "    b: bias array of shape (C_out,)\n",
    "    Returns: output feature map of shape (C_out, H_out, W_out)\n",
    "\n",
    "    TODO: use numpy for simplifying for loops\n",
    "    \"\"\"\n",
    "    C_out, C_in, kH, kW = W.shape\n",
    "    _, H_in, W_in = X.shape\n",
    "    H_out = H_in - kH + 1  # using stride 1, no padding\n",
    "    W_out = W_in - kW + 1\n",
    "    # Initialize output volume\n",
    "    out = np.zeros((C_out, H_out, W_out))\n",
    "    # Convolution: slide each filter over the input\n",
    "    for oc in range(C_out):             # for each output channel (filter)\n",
    "        for i in range(H_out):          # slide vertically\n",
    "            for j in range(W_out):      # slide horizontally\n",
    "                # current region of input of shape (C_in, kH, kW)\n",
    "                region = X[:, i:i+kH, j:j+kW]\n",
    "                # element-wise multiply and sum -> dot product\n",
    "                out[oc, i, j] = np.sum(region * W[oc]) + b[oc]\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "# X has shape (1, 32, 32) for a single grayscale image, W shape (6, 1, 5, 5), b shape (6,)\n",
    "X = np.random.rand(1, 32, 32)  # dummy input\n",
    "W1 = np.random.rand(6, 1, 5, 5)\n",
    "b1 = np.random.rand(6,)\n",
    "out1 = conv_forward(X, W1, b1)\n",
    "print(out1)  # should be (6, 28, 28)\n",
    "\n",
    "out1_act = tanh(out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "def avg_pool_forward(X):\n",
    "    \"\"\"\n",
    "    Average pooling forward pass (2x2 pool, stride 2).\n",
    "    X: input array of shape (C, H, W)\n",
    "    Returns: output array of shape (C, H/2, W/2)\n",
    "    \"\"\"\n",
    "    C, H, W = X.shape\n",
    "    # Assuming H and W are even and divisible by 2 for simplicity\n",
    "    out = np.zeros((C, H//2, W//2))\n",
    "    for c in range(C):\n",
    "        for i in range(0, H, 2):         # step by 2\n",
    "            for j in range(0, W, 2):     # step by 2\n",
    "                patch = X[c, i:i+2, j:j+2]        # 2x2 region\n",
    "                out[c, i//2, j//2] = np.mean(patch)\n",
    "    return out\n",
    "\n",
    "# Example usage:\n",
    "X_pool_in = np.random.rand(6, 28, 28)  # e.g. output from conv layer (6,28,28)\n",
    "out_pool = avg_pool_forward(X_pool_in)\n",
    "print(out_pool.shape)  # should be (6, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84,)\n"
     ]
    }
   ],
   "source": [
    "def fc_forward(x, W, b):\n",
    "    \"\"\"\n",
    "    Fully-connected layer forward pass.\n",
    "    x: input vector of shape (N_in,)\n",
    "    W: weight matrix of shape (N_out, N_in)\n",
    "    b: bias vector of shape (N_out,)\n",
    "    Returns: output vector of shape (N_out,)\n",
    "    \"\"\"\n",
    "    return W.dot(x) + b\n",
    "\n",
    "# Example usage:\n",
    "x = np.random.rand(120,)    # input from previous layer (flattened conv output)\n",
    "W4 = np.random.rand(84, 120)\n",
    "b4 = np.random.rand(84,)\n",
    "out_fc = fc_forward(x, W4, b4)\n",
    "print(out_fc.shape)  # should be (84,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_forward(image):\n",
    "    \"\"\"\n",
    "    Forward pass for the entire LeNet-5 model on a single image.\n",
    "    image: input image array of shape (1, 32, 32)  - 1 channel, 32x32 (assuming already zero-padded to 32x32 if originally 28x28).\n",
    "    Returns: output probabilities for 10 classes (softmax output vector of shape (10,))\n",
    "    \"\"\"\n",
    "    # Layer C1: Conv 5x5 -> 6 feature maps, then activation\n",
    "    out_c1 = conv_forward(image, W1, b1)         # shape (6, 28, 28)\n",
    "    out_c1 = tanh(out_c1)                        # apply tanh or relu\n",
    "    \n",
    "    # Layer S2: 2x2 Pooling -> 6 feature maps\n",
    "    out_s2 = avg_pool_forward(out_c1)            # shape (6, 14, 14)\n",
    "    \n",
    "    # Layer C3: Conv 5x5 -> 16 feature maps, then activation\n",
    "    out_c3 = conv_forward(out_s2, W2, b2)        # shape (16, 10, 10)\n",
    "    out_c3 = tanh(out_c3)\n",
    "    \n",
    "    # Layer S4: 2x2 Pooling -> 16 feature maps\n",
    "    out_s4 = avg_pool_forward(out_c3)            # shape (16, 5, 5)\n",
    "    \n",
    "    # Layer C5: Conv 5x5 -> 120 feature maps (1x1 each), then activation\n",
    "    out_c5 = conv_forward(out_s4, W3, b3)        # shape (120, 1, 1)\n",
    "    out_c5 = tanh(out_c5)\n",
    "    # Flatten output of C5 to a vector of length 120:\n",
    "    out_c5_flat = out_c5.reshape(-1)             # shape (120,)\n",
    "    \n",
    "    # Layer F6: Fully connected -> 84, then activation\n",
    "    out_f6 = fc_forward(out_c5_flat, W4, b4)     # shape (84,)\n",
    "    out_f6 = tanh(out_f6)\n",
    "    \n",
    "    # Output layer: Fully connected -> 10, then softmax\n",
    "    out_out = fc_forward(out_f6, W5, b5)         # shape (10,)\n",
    "    probs = softmax(out_out)                     # shape (10,) probabilities\n",
    "    \n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 120) (84,) (120,)\n"
     ]
    }
   ],
   "source": [
    "def fc_backward(dout, x, W):\n",
    "    \"\"\"\n",
    "    Backprop for a fully-connected layer.\n",
    "    dout: gradient of loss w.r.t. layer output (shape: N_out,)\n",
    "    x: input to the layer from forward pass (shape: N_in,)\n",
    "    W: weight matrix of this layer (shape: N_out, N_in)\n",
    "    Returns: (dx, dW, db)\n",
    "    \"\"\"\n",
    "    # Gradient w.rt input\n",
    "    dx = W.T.dot(dout)         # shape: (N_in,)\n",
    "    # Gradient w.rt weights\n",
    "    dW = np.outer(dout, x)     # outer product of dout (N_out,) and x (N_in,) to get (N_out, N_in)\n",
    "    # Gradient w.rt bias\n",
    "    db = dout.copy()           # shape: (N_out,) (for a single sample, just copy dout; for batch, we'd sum dout over batch)\n",
    "    return dx, dW, db\n",
    "\n",
    "# Example usage:\n",
    "dout = np.random.rand(84,)   # say gradient from next layer for F6 output (84,)\n",
    "x = np.random.rand(120,)     # the input that was fed into this FC layer (flattened conv output)\n",
    "W4 = np.random.rand(84, 120)\n",
    "dx, dW, db = fc_backward(dout, x, W4)\n",
    "print(dW.shape, db.shape, dx.shape)  # (84, 120), (84,), (120,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1, 5, 5) (6,) (1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "def conv_backward(dout, X, W):\n",
    "    \"\"\"\n",
    "    Backprop for a convolutional layer (stride=1, no padding).\n",
    "    dout: gradient of loss w.rt. conv layer output (shape: C_out, H_out, W_out)\n",
    "    X: input to the conv layer from forward pass (shape: C_in, H_in, W_in)\n",
    "    W: conv layer weight matrix (shape: C_out, C_in, kH, kW)\n",
    "    Returns: (dX, dW, db)\n",
    "    \"\"\"\n",
    "    C_out, C_in, kH, kW = W.shape\n",
    "    _, H_in, W_in = X.shape\n",
    "    _, H_out, W_out = dout.shape\n",
    "\n",
    "    # Initialize gradients to zero\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros(C_out)\n",
    "    dX = np.zeros_like(X)\n",
    "    # Compute gradients\n",
    "    for oc in range(C_out):\n",
    "        # Bias grad: sum of dout for this output channel\n",
    "        db[oc] += np.sum(dout[oc])\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                # The gradient from this output element\n",
    "                grad_val = dout[oc, i, j]\n",
    "                # The corresponding input region that contributed to this output\n",
    "                input_region = X[:, i:i+kH, j:j+kW]\n",
    "                # Weight gradients: add input_region * grad_val\n",
    "                dW[oc] += grad_val * input_region\n",
    "                # Input gradients: add filter weights * grad_val\n",
    "                dX[:, i:i+kH, j:j+kW] += grad_val * W[oc]\n",
    "    return dX, dW, db\n",
    "\n",
    "# Example usage:\n",
    "# dout has shape (6, 28, 28) matching conv1 output, X was (1, 32, 32), W1 was (6, 1, 5, 5)\n",
    "dout = np.random.rand(6, 28, 28)\n",
    "X = np.random.rand(1, 32, 32)\n",
    "W1 = np.random.rand(6, 1, 5, 5)\n",
    "dX, dW, db = conv_backward(dout, X, W1)\n",
    "print(dW.shape, db.shape, dX.shape)  # (6,1,5,5), (6,), (1,32,32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def avg_pool_backward(dout, X):\n",
    "    \"\"\"\n",
    "    Backprop for 2x2 average pooling layer.\n",
    "    dout: gradient of loss w.rt pooling output (shape: C, H_out, W_out)\n",
    "    X: input to pooling layer from forward pass (shape: C, H_in, W_in)\n",
    "    Returns: dX (same shape as X)\n",
    "    \"\"\"\n",
    "    C, H_in, W_in = X.shape\n",
    "    # H_out, W_out are half of H_in, W_in\n",
    "    dX = np.zeros_like(X)\n",
    "    # Each output gradient splits evenly to 4 inputs\n",
    "    for c in range(C):\n",
    "        for i in range(0, H_in, 2):\n",
    "            for j in range(0, W_in, 2):\n",
    "                # gradient for this 2x2 block in output\n",
    "                grad_val = dout[c, i//2, j//2]\n",
    "                # distribute it to each of the 4 input cells\n",
    "                dX[c, i:i+2, j:j+2] += grad_val / 4.0\n",
    "    return dX\n",
    "\n",
    "# Example usage:\n",
    "dout = np.random.rand(6, 14, 14)  # gradient from next layer (S2 output shape)\n",
    "X = np.random.rand(6, 28, 28)     # original input to pooling (C1 output)\n",
    "dX = avg_pool_backward(dout, X)\n",
    "print(dX.shape)  # (6, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine fan_in and fan_out for each layer\n",
    "# Layer C1: 6 filters of size 5x5x1 -> fan_in = 1*5*5, fan_out = 6*5*5 (if considering each filter connects to 5x5 region in one output)\n",
    "fan_in_c1 = 1 * 5 * 5\n",
    "fan_out_c1 = 6 * 5 * 5\n",
    "# Using Xavier (tanh) for conv layers as in original LeNet\n",
    "W1 = np.random.randn(6, 1, 5, 5) * np.sqrt(2.0 / (fan_in_c1 + fan_out_c1))\n",
    "b1 = np.zeros(6)\n",
    "\n",
    "# Layer C3: 16 filters of size 5x5x6\n",
    "fan_in_c3 = 6 * 5 * 5\n",
    "fan_out_c3 = 16 * 5 * 5\n",
    "W2 = np.random.randn(16, 6, 5, 5) * np.sqrt(2.0 / (fan_in_c3 + fan_out_c3))\n",
    "b2 = np.zeros(16)\n",
    "\n",
    "# Layer C5: 120 filters of size 5x5x16\n",
    "fan_in_c5 = 16 * 5 * 5\n",
    "fan_out_c5 = 120 * 5 * 5\n",
    "W3 = np.random.randn(120, 16, 5, 5) * np.sqrt(2.0 / (fan_in_c5 + fan_out_c5))\n",
    "b3 = np.zeros(120)\n",
    "\n",
    "# Fully connected F6: 120 -> 84\n",
    "fan_in_f6 = 120\n",
    "fan_out_f6 = 84\n",
    "W4 = np.random.randn(84, 120) * np.sqrt(2.0 / (fan_in_f6 + fan_out_f6))\n",
    "b4 = np.zeros(84)\n",
    "\n",
    "# Output layer: 84 -> 10\n",
    "fan_in_out = 84\n",
    "fan_out_out = 10\n",
    "W5 = np.random.randn(10, 84) * np.sqrt(2.0 / (fan_in_out + fan_out_out))\n",
    "b5 = np.zeros(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: Raw model outputs of shape (batch_size, num_classes)\n",
    "    - y_true: True labels, can be either:\n",
    "              - Class indices of shape (batch_size,)\n",
    "              - One-hot encoded vectors of shape (batch_size, num_classes)\n",
    "    \n",
    "    Returns:\n",
    "    - Average cross-entropy loss over the batch\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    # Shift by max for numerical stability\n",
    "    shifted_logits = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(shifted_logits)\n",
    "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    batch_size = logits.shape[0]\n",
    "    \n",
    "    # Handle both one-hot and class index formats\n",
    "    if len(y_true.shape) == 1:\n",
    "        # y_true contains class indices\n",
    "        batch_indices = np.arange(batch_size)\n",
    "        true_probs = probs[batch_indices, y_true]\n",
    "        loss = -np.mean(np.log(true_probs + 1e-12))  # Add epsilon to avoid log(0)\n",
    "    else:\n",
    "        # y_true is one-hot encoded\n",
    "        loss = -np.sum(y_true * np.log(probs + 1e-12)) / batch_size\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data shape: (60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset (it returns train and test splits)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"MNIST data shape:\", X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5000 training examples instead of 60000\n",
      "Using 5000 test examples instead of 10000\n",
      "After padding: (5000, 32, 32) (5000, 32, 32)\n",
      "After adding channel dim: (5000, 1, 32, 32) (5000, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Normalize pixel values to 0-1\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Select a smaller subset of training data\n",
    "train_subset_size = 5000  # Adjust as needed\n",
    "indices_train = np.random.choice(X_train.shape[0], train_subset_size, replace=False)\n",
    "X_train = X_train[indices_train]\n",
    "y_train = y_train[indices_train]\n",
    "print(f\"Using {train_subset_size} training examples instead of 60000\")\n",
    "\n",
    "# Select a smaller subset of test data\n",
    "test_subset_size = 5000  # Adjust as needed\n",
    "indices_test = np.random.choice(X_test.shape[0], test_subset_size, replace=False)\n",
    "X_test = X_test[indices_test]\n",
    "y_test = y_test[indices_test]\n",
    "print(f\"Using {test_subset_size} test examples instead of 10000\")\n",
    "\n",
    "# Pad images from 28x28 to 32x32\n",
    "X_train_padded = np.pad(X_train, ((0,0),(2,2),(2,2)), mode='constant')\n",
    "X_test_padded = np.pad(X_test, ((0,0),(2,2),(2,2)), mode='constant')\n",
    "print(\"After padding:\", X_train_padded.shape, X_test_padded.shape)\n",
    "\n",
    "# Reshape to add channel dimension\n",
    "X_train_padded = X_train_padded.reshape(-1, 1, 32, 32)\n",
    "X_test_padded = X_test_padded.reshape(-1, 1, 32, 32)\n",
    "print(\"After adding channel dim:\", X_train_padded.shape, X_test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: average loss = 0.5404\n",
      "Epoch 2: average loss = 0.2573\n",
      "Epoch 3: average loss = 0.1679\n",
      "Epoch 4: average loss = 0.1143\n",
      "Epoch 5: average loss = 0.0725\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle training data (optional for SGD)\n",
    "    permutation = np.random.permutation(X_train_padded.shape[0])\n",
    "    X_train_padded = X_train_padded[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for n in range(X_train_padded.shape[0]):\n",
    "        x = X_train_padded[n]        # shape (1,32,32)\n",
    "        y = y_train[n]              # true label 0-9\n",
    "        \n",
    "        # ===== Forward pass =====\n",
    "        out_c1 = tanh(conv_forward(x, W1, b1))\n",
    "        out_s2 = avg_pool_forward(out_c1)\n",
    "        out_c3 = tanh(conv_forward(out_s2, W2, b2))\n",
    "        out_s4 = avg_pool_forward(out_c3)\n",
    "        out_c5 = tanh(conv_forward(out_s4, W3, b3))    # shape (120,1,1)\n",
    "        out_c5_flat = out_c5.reshape(-1)               # flatten to (120,)\n",
    "        out_f6 = tanh(fc_forward(out_c5_flat, W4, b4)) # shape (84,)\n",
    "        out_final = fc_forward(out_f6, W5, b5)         # shape (10,)\n",
    "        probs = softmax(out_final)                    # shape (10,)\n",
    "        \n",
    "        # Compute cross-entropy loss and initial gradient\n",
    "        # Create one-hot vector for true label\n",
    "        y_onehot = np.zeros(10)\n",
    "        y_onehot[y] = 1.0\n",
    "        # Loss for this sample (cross-entropy): -sum(y_onehot * log(probs))\n",
    "        loss = -np.sum(y_onehot * np.log(probs + 1e-8))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Gradient of loss w.rt. pre-softmax scores\n",
    "        dout = probs - y_onehot  # shape (10,)\n",
    "        \n",
    "        # ===== Backward pass =====\n",
    "        # Backprop through output layer (FC 10)\n",
    "        d_out_f6, dW5, db5 = fc_backward(dout, out_f6, W5)\n",
    "        # Backprop through activation (tanh) at F6\n",
    "        d_out_f6 *= (1 - out_f6**2)\n",
    "        \n",
    "        # F6 layer (FC 84) backward\n",
    "        d_out_c5_flat, dW4, db4 = fc_backward(d_out_f6, out_c5_flat, W4)\n",
    "        # Backprop through activation (tanh) at C5 output\n",
    "        out_c5_vec = out_c5.reshape(-1)       # 120,\n",
    "        d_out_c5_vec = d_out_c5_flat * (1 - out_c5_vec**2)\n",
    "        d_out_c5 = d_out_c5_vec.reshape(120, 1, 1)  # reshape to (120,1,1) to match conv output shape\n",
    "        \n",
    "        # C5 conv layer backward\n",
    "        d_out_s4, dW3, db3 = conv_backward(d_out_c5, out_s4, W3)\n",
    "        # Backprop through activation (tanh) at C3 output\n",
    "        d_out_s4 = d_out_s4  # S4 output was before activation, actually C5 input = S4 output, no activation directly on S4.\n",
    "        \n",
    "        # S4 pooling backward\n",
    "        d_out_c3 = avg_pool_backward(d_out_s4, out_c3)\n",
    "        # Backprop through activation (tanh) at C3\n",
    "        d_out_c3 *= (1 - out_c3**2)\n",
    "        \n",
    "        # C3 conv layer backward\n",
    "        d_out_s2, dW2, db2 = conv_backward(d_out_c3, out_s2, W2)\n",
    "        # S2 pooling backward\n",
    "        d_out_c1 = avg_pool_backward(d_out_s2, out_c1)\n",
    "        # Backprop through activation (tanh) at C1\n",
    "        d_out_c1 *= (1 - out_c1**2)\n",
    "        \n",
    "        # C1 conv layer backward\n",
    "        _, dW1, db1 = conv_backward(d_out_c1, x, W1)  # we don't need dX for input layer further\n",
    "        \n",
    "        # ===== Update weights with SGD =====\n",
    "        W5 -= learning_rate * dW5; b5 -= learning_rate * db5\n",
    "        W4 -= learning_rate * dW4; b4 -= learning_rate * db4\n",
    "        W3 -= learning_rate * dW3; b3 -= learning_rate * db3\n",
    "        W2 -= learning_rate * dW2; b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1; b1 -= learning_rate * db1\n",
    "    # End of epoch\n",
    "    avg_loss = total_loss / X_train_padded.shape[0]\n",
    "    print(f\"Epoch {epoch+1}: average loss = {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 95.80%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = X_test_padded.shape[0]\n",
    "for n in range(total):\n",
    "    x = X_test_padded[n]\n",
    "    y_true = y_test[n]\n",
    "    # Forward pass (using the same lenet5_forward or expanded steps as before)\n",
    "    probs = lenet5_forward(x)         # get softmax probabilities for this test image\n",
    "    y_pred = np.argmax(probs)         # predicted class is index of max probability\n",
    "    if y_pred == y_true:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy = {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 4\n",
      "Confidence: 0.9943\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./image.png\"\n",
    "img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "# 2. Resize to 28x28 (MNIST standard size)\n",
    "img = img.resize((28, 28))\n",
    "\n",
    "# 3. Convert to numpy array and normalize\n",
    "img_array = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "# 4. Invert colors if needed (assuming black digit on white background)\n",
    "# MNIST has white digits on black background\n",
    "img_array = 1.0 - img_array  # Comment this out if your image is already white digit on black\n",
    "\n",
    "# 6. Pad to 32x32 (to match your model's expected input)\n",
    "img_padded = np.pad(img_array, ((2, 2), (2, 2)), mode='constant')\n",
    "\n",
    "img_input = img_padded.reshape(1, 32, 32)\n",
    "\n",
    "probs = lenet5_forward(img_input)\n",
    "y_pred = np.argmax(probs)\n",
    "\n",
    "# 5. Display prediction result\n",
    "print(f\"Model prediction: {y_pred}\")\n",
    "print(f\"Confidence: {probs[y_pred]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
